# âœ… å®Œæ•´ç‰ˆï¼šæ”¹è‰¯å¾Œçš„è¨“ç·´è…³æœ¬ with BIO æª¢æŸ¥ã€Offset å°é½Šã€é¡åˆ¥åŠ æ¬Š

import os
import json
import re
from datasets import Dataset
from transformers import DebertaV2TokenizerFast, DebertaV2ForTokenClassification, TrainingArguments, Trainer,TrainerCallback
import torch
import torch.nn as nn
from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score

ALL_SHI_TYPES = ["DOCTOR"]
LABELS = ["O"] + [f"{prefix}-{t}" for t in ALL_SHI_TYPES for prefix in ("B", "I")]
label2id = {label: i for i, label in enumerate(LABELS)}
id2label = {i: label for label, i in label2id.items()}

class SaveEvalResultsCallback(TrainerCallback):
    def on_evaluate(self, args, state, control, metrics, **kwargs):
        os.makedirs("training_eval", exist_ok=True)
        eval_file = "training_eval/eval_result_doctor.json"
        epoch_number = int(state.epoch) if state.epoch is not None else 0
        eval_entry = {
            "epoch": epoch_number,
            **metrics  
        }

        if os.path.exists(eval_file):
            with open(eval_file, "r", encoding="utf-8") as f:
                all_results = json.load(f)
                if not isinstance(all_results, list):
                    all_results = [all_results]
        else:
            all_results = []

        all_results.append(eval_entry)

        with open(eval_file, "w", encoding="utf-8") as f:
            json.dump(all_results, f, indent=4, ensure_ascii=False)

        print(f"[Callback] å„²å­˜ eval çµæœï¼ˆepoch={epoch_number}ï¼‰è‡³ {eval_file}")


tokenizer = DebertaV2TokenizerFast.from_pretrained("microsoft/deberta-v3-base")

def validate_bio_sequence(bio_tags):
    for i, tag in enumerate(bio_tags):
        if tag.startswith("I-") and (i == 0 or bio_tags[i - 1][2:] != tag[2:] or not bio_tags[i - 1].startswith(("B-", "I-"))):
            print(f" BIOéŒ¯èª¤ at position {i}: {tag} â† {bio_tags[i - 1]}")

def char_level_bio_encoding(text, entities):
    labels = ["O"] * len(text)
    for start, end, label in entities:
        if end > len(text) or start >= end:
            continue
        labels[start] = f"B-{label}"
        for i in range(start + 1, end):
            if i < len(labels):
                labels[i] = f"I-{label}"
    validate_bio_sequence(labels)
    return labels

def align_labels_with_offsets(char_labels, offset_mapping):
    labels = []
    for start, end in offset_mapping:
        if start == end:
            labels.append(-100)
        else:
            span_labels = set(char_labels[start:end])
            span_labels.discard("O")
            if span_labels:
                tag = sorted(span_labels)[0]
                labels.append(label2id.get(tag, label2id["O"]))
            else:
                labels.append(label2id["O"])
    return labels

def load_data_char_based(task1_path, task2_path):
    with open(task1_path, encoding='utf-8') as f:
        texts = dict(line.strip().split('\t', 1) for line in f if line.strip())

    annotations = {}
    with open(task2_path, encoding='utf-8') as f:
        for line in f:
            parts = line.strip().split('\t')
            if len(parts) != 5:
                print(f" ç„¡æ³•è§£ææ­¤è¡Œï¼ˆé æœŸ5å€‹é …ç›®ï¼‰: {line}")
                continue  
            try:
                sid, label, start, end, _ = parts
                annotations.setdefault(sid, []).append((int(float(start)), int(float(end)), label))
            except ValueError as e:
                print(f" è§£æéŒ¯èª¤è¡Œ: {line} - éŒ¯èª¤: {e}")
                continue  

    dataset = []
    for sid, text in texts.items():
        entity_list = annotations.get(sid, [])
        char_labels = char_level_bio_encoding(text, entity_list)

        encoded = tokenizer(text, return_offsets_mapping=True, padding="max_length", truncation=True, max_length=256)
        encoded["labels"] = align_labels_with_offsets(char_labels, encoded["offset_mapping"])
        encoded.pop("offset_mapping")
        dataset.append(encoded)

    return dataset


class WeightedNERTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs): 
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.logits

        weights = torch.ones(len(label2id)).to(logits.device)
        weights[label2id["O"]] = 0.1

        loss_fct = nn.CrossEntropyLoss(weight=weights, ignore_index=-100)
        loss = loss_fct(logits.view(-1, len(label2id)), labels.view(-1))
        return (loss, outputs) if return_outputs else loss

def compute_metrics(p):
    preds = p.predictions.argmax(-1)
    labels = p.label_ids
    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]
    true_preds = [[id2label[p] for (p, l) in zip(pred, label) if l != -100] for pred, label in zip(preds, labels)]
    return {
        "precision": precision_score(true_labels, true_preds),
        "recall": recall_score(true_labels, true_preds),
        "f1": f1_score(true_labels, true_preds),
        "accuracy": accuracy_score(true_labels, true_preds)
    }

train_data = load_data_char_based("train_data/task1_doctor.txt", "train_data/task2_doctor.txt")
dataset = Dataset.from_list(train_data).train_test_split(test_size=0.2, seed=42)

model = DebertaV2ForTokenClassification.from_pretrained(
    "microsoft/deberta-v3-base",
    num_labels=len(label2id),
    id2label=id2label,
    label2id=label2id
)

args = TrainingArguments(
    output_dir="./ner_model",
    evaluation_strategy="epoch", 
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=20,
    weight_decay=0.01,
    logging_dir="./logs/log_doctor",
    save_total_limit=1,
    logging_steps=500
)

trainer = WeightedNERTrainer(
    model=model,
    args=args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.add_callback(SaveEvalResultsCallback())


last_checkpoint = None
if os.path.isdir(args.output_dir):
    checkpoints = [os.path.join(args.output_dir, d) for d in os.listdir(args.output_dir) if d.startswith("checkpoint-")]
    if checkpoints:
        last_checkpoint = sorted(checkpoints)[-1]
        print(f"ğŸŒ€ æ‰¾åˆ°ç¾æœ‰ checkpointï¼Œå¯å¾é€™è£¡æ¢å¾©è¨“ç·´ï¼š{last_checkpoint}")
    else:
        print("ğŸ†• æ²’æœ‰ checkpointï¼Œå°‡å¾é ­é–‹å§‹è¨“ç·´")
else:
    print(" å°šæœªå»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾ï¼Œå°‡å¾é ­é–‹å§‹è¨“ç·´")


trainer.train(resume_from_checkpoint=last_checkpoint)

model.save_pretrained("model/ner_model_doctor")
tokenizer.save_pretrained("model/ner_model_doctor")

print("æ¨¡å‹è¨“ç·´èˆ‡å„²å­˜å®Œæˆï¼")